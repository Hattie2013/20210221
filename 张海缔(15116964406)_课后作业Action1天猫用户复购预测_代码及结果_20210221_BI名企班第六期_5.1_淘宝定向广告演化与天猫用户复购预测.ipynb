{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./train_format2.csv does not exist: './train_format2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-268826f1ab87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_data1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./train_format2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_data1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'origin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./train_format2.csv does not exist: './train_format2.csv'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "# 用户行为，使用format1进行加载\n",
    "# 加载全量样本\n",
    "\"\"\"\n",
    "user_log = pd.read_csv('./data_format1/user_log_format1.csv', dtype={'time_stamp':'str'})\n",
    "user_info = pd.read_csv('./data_format1/user_info_format1.csv')\n",
    "train_data1 = pd.read_csv('./data_format1/train_format1.csv')\n",
    "submission = pd.read_csv('./data_format1/test_format1.csv')\n",
    "\"\"\"\n",
    "# 加载小样本\n",
    "user_log = pd.read_csv('./sample_user_log.csv', dtype={'time_stamp':'str'})\n",
    "user_info = pd.read_csv('./sample_user_info.csv')\n",
    "train_data1 = pd.read_csv('./train.csv')\n",
    "submission = pd.read_csv('./test.csv')\n",
    "train_data = pd.read_csv('./train_format2.csv')\n",
    "\n",
    "train_data1['origin'] = 'train'\n",
    "submission['origin'] = 'test'\n",
    "matrix = pd.concat([train_data1, submission], ignore_index=True, sort=False)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.drop(['prob'], axis=1, inplace=True)\n",
    "# 连接user_info表，通过user_id关联\n",
    "matrix = matrix.merge(user_info, on='user_id', how='left')\n",
    "# 使用merchant_id（原列名seller_id）\n",
    "user_log.rename(columns={'seller_id':'merchant_id'}, inplace=True)\n",
    "# 格式化\n",
    "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
    "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
    "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
    "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
    "user_log['brand_id'].fillna(0, inplace=True)\n",
    "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
    "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')\n",
    "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
    "matrix['age_range'].fillna(0, inplace=True)\n",
    "# 0:female, 1:male, 2:unknown\n",
    "matrix['gender'].fillna(2, inplace=True)\n",
    "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
    "matrix['gender'] = matrix['gender'].astype('int8')\n",
    "matrix['label'] = matrix['label'].astype('str')\n",
    "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
    "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
    "del user_info, train_data1\n",
    "gc.collect()\n",
    "#print(matrix)\n",
    "# User特征处理\n",
    "groups = user_log.groupby(['user_id'])\n",
    "# 用户交互行为数量 u1\n",
    "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "# 使用agg 基于列的聚合操作，统计唯一值的个数 item_id, cat_id, merchant_id, brand_id\n",
    "#temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id':'u4', 'brand_id':'u5'})\n",
    "# 对于每个user_id 不重复的item_id的数量 => u2\n",
    "temp = groups['item_id'].agg([('u2', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "# 对于每个user_id 不重复的cat_id的数量 => u3\n",
    "temp = groups['cat_id'].agg([('u3', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['merchant_id'].agg([('u4', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['brand_id'].agg([('u5', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "# 时间间隔特征 u6 按照小时\n",
    "# 对于每个user_id 计算time_stamp的最小时间 => F_time, 最大时间max => L_time\n",
    "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
    "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600\n",
    "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
    "# 统计操作类型为0，1，2，3的个数\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "#print(matrix)\n",
    "\n",
    "# 商家特征处理\n",
    "groups = user_log.groupby(['merchant_id'])\n",
    "# 商家被交互行为数量 m1\n",
    "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
    "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的action_type 唯一值\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 按照merchant_id 统计随机负采样的个数\n",
    "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照user_id, merchant_id分组\n",
    "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
    "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
    "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
    "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
    "#print(matrix)\n",
    "\n",
    "#用户购买点击比\n",
    "matrix['r1'] = matrix['u9']/matrix['u7'] \n",
    "#商家购买点击比\n",
    "matrix['r2'] = matrix['m8']/matrix['m6'] \n",
    "#不同用户不同商家购买点击比\n",
    "matrix['r3'] = matrix['um7']/matrix['um5']\n",
    "matrix.fillna(0, inplace=True)\n",
    "# # 修改age_range字段名称为 age_0, age_1, age_2... age_8\n",
    "temp = pd.get_dummies(matrix['age_range'], prefix='age')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "temp = pd.get_dummies(matrix['gender'], prefix='g')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练数据和测试数据\n",
    "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
    "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
    "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']\n",
    "del temp, matrix\n",
    "gc.collect()\n",
    "\n",
    "# 使用机器学习工具\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "# 将训练集进行切分，20%用于验证\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=.2)\n",
    "\n",
    "# 使用XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth=8,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42    \n",
    ")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_metric='auc', eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    verbose=True,\n",
    "    #早停法，如果auc在10epoch没有进步就stop\n",
    "    early_stopping_rounds=10 \n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "prob = model.predict_proba(test_data)\n",
    "submission['prob'] = pd.Series(prob[:,1])\n",
    "submission.drop(['origin'], axis=1, inplace=True)\n",
    "submission.to_csv('prediction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
